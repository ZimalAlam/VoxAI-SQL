#!/bin/bash

echo "ğŸ¤– Setting up Ollama for VoxAI Conversational AI"
echo "================================================"

# Check if Ollama is already installed
if command -v ollama &> /dev/null; then
    echo "âœ… Ollama is already installed"
else
    echo "ğŸ“¥ Installing Ollama..."
    curl -fsSL https://ollama.com/install.sh | sh
    
    if [ $? -eq 0 ]; then
        echo "âœ… Ollama installed successfully"
    else
        echo "âŒ Failed to install Ollama"
        exit 1
    fi
fi

echo ""
echo "ğŸš€ Starting Ollama service..."
ollama serve &
OLLAMA_PID=$!

# Wait for Ollama to start
echo "â³ Waiting for Ollama to start..."
sleep 5

# Check if Ollama is running
if curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "âœ… Ollama is running"
else
    echo "âŒ Ollama failed to start"
    exit 1
fi

echo ""
echo "ğŸ“¦ Installing recommended AI models..."

# Install lightweight models for better performance
echo "ğŸ“¥ Installing Llama 3.2 1B (lightweight, fast)..."
ollama pull llama3.2:1b

echo "ğŸ“¥ Installing Phi-3 Mini (very lightweight)..."
ollama pull phi3:mini

echo "ğŸ“¥ Installing Mistral 7B (more capable, slower)..."
ollama pull mistral:7b

echo ""
echo "ğŸ§ª Testing models..."

# Test the models
echo "Testing Llama 3.2..."
echo "Hello, how are you?" | ollama run llama3.2:1b

echo ""
echo "âœ… Setup complete!"
echo ""
echo "ğŸ¯ Available models:"
ollama list

echo ""
echo "ğŸš€ To start the conversational AI service:"
echo "   cd conversational-ai"
echo "   pip install -r requirements.txt"
echo "   python3 app.py"
echo ""
echo "ğŸ“ The service will run on http://localhost:5004"
echo "ğŸ”— Ollama API is available at http://localhost:11434" 