#!/bin/bash

echo "ü§ñ Setting up Ollama for VoxAI Conversational AI (macOS)"
echo "====================================================="

# Check if we're on macOS
if [[ "$OSTYPE" != "darwin"* ]]; then
    echo "‚ùå This script is for macOS only. For Linux, use setup_ollama.sh"
    exit 1
fi

# Check if Ollama is already installed
if command -v ollama &> /dev/null; then
    echo "‚úÖ Ollama is already installed"
    ollama --version
else
    echo "üì• Installing Ollama for macOS..."
    
    # Check if we have curl
    if ! command -v curl &> /dev/null; then
        echo "‚ùå curl is required but not installed"
        exit 1
    fi
    
    # Download and install Ollama for macOS
    echo "üîÑ Downloading Ollama..."
    curl -L https://ollama.com/download/ollama-darwin -o /tmp/ollama
    
    if [ $? -eq 0 ]; then
        echo "üîß Installing Ollama..."
        chmod +x /tmp/ollama
        sudo mv /tmp/ollama /usr/local/bin/ollama
        
        if [ $? -eq 0 ]; then
            echo "‚úÖ Ollama installed successfully"
        else
            echo "‚ùå Failed to install Ollama (permission denied?)"
            echo "üí° Try running: sudo mv /tmp/ollama /usr/local/bin/ollama"
            exit 1
        fi
    else
        echo "‚ùå Failed to download Ollama"
        echo "üí° You can also install via Homebrew: brew install ollama"
        exit 1
    fi
fi

echo ""
echo "üöÄ Starting Ollama service..."

# Start Ollama in background
ollama serve > /tmp/ollama.log 2>&1 &
OLLAMA_PID=$!

echo "   Ollama PID: $OLLAMA_PID"
echo "   Log file: /tmp/ollama.log"

# Wait for Ollama to start
echo "‚è≥ Waiting for Ollama to start..."
sleep 8

# Check if Ollama is running
MAX_ATTEMPTS=10
ATTEMPT=1

while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "‚úÖ Ollama is running on http://localhost:11434"
        break
    else
        echo "   Attempt $ATTEMPT/$MAX_ATTEMPTS - waiting..."
        sleep 2
        ATTEMPT=$((ATTEMPT + 1))
    fi
done

if [ $ATTEMPT -gt $MAX_ATTEMPTS ]; then
    echo "‚ùå Ollama failed to start after $MAX_ATTEMPTS attempts"
    echo "üìã Check the log file: tail -f /tmp/ollama.log"
    exit 1
fi

echo ""
echo "üì¶ Installing recommended AI models..."
echo "‚ÑπÔ∏è  This may take several minutes depending on your internet connection"

# Install lightweight models for better performance on macOS
echo ""
echo "üì• Installing Llama 3.2 1B (lightweight, ~1GB)..."
ollama pull llama3.2:1b
if [ $? -eq 0 ]; then
    echo "   ‚úÖ Llama 3.2 1B installed"
else
    echo "   ‚ùå Failed to install Llama 3.2 1B"
fi

echo ""
echo "üì• Installing Phi-3 Mini (very lightweight, ~2GB)..."
ollama pull phi3:mini
if [ $? -eq 0 ]; then
    echo "   ‚úÖ Phi-3 Mini installed"
else
    echo "   ‚ùå Failed to install Phi-3 Mini"
fi

echo ""
echo "üì• Installing Mistral 7B (more capable, ~4GB)..."
echo "‚ö†Ô∏è  This is a larger model and may take longer to download"
ollama pull mistral:7b
if [ $? -eq 0 ]; then
    echo "   ‚úÖ Mistral 7B installed"
else
    echo "   ‚ùå Failed to install Mistral 7B (you can install it later)"
fi

echo ""
echo "üß™ Testing models..."

# Test the primary model
echo "Testing Llama 3.2 1B..."
TEST_RESPONSE=$(echo "Hello, how are you?" | ollama run llama3.2:1b 2>/dev/null | head -1)
if [ -n "$TEST_RESPONSE" ]; then
    echo "   ‚úÖ Test successful: $TEST_RESPONSE"
else
    echo "   ‚ö†Ô∏è  Test response was empty (model may still be loading)"
fi

echo ""
echo "‚úÖ Setup complete!"
echo ""
echo "üéØ Available models:"
ollama list

echo ""
echo "üì± Next steps:"
echo "   1. Install Python dependencies:"
echo "      cd conversational-ai"
echo "      pip3 install -r requirements.txt"
echo ""
echo "   2. Start the conversational AI service:"
echo "      python3 app.py"
echo ""
echo "   3. Or start all VoxAI services at once:"
echo "      cd .."
echo "      ./start_voxai_with_ai.sh"
echo ""
echo "üìù Service URLs:"
echo "   ‚Ä¢ Conversational AI: http://localhost:5004"
echo "   ‚Ä¢ Ollama API:        http://localhost:11434"
echo ""
echo "üîß Troubleshooting:"
echo "   ‚Ä¢ Check Ollama status: curl http://localhost:11434/api/tags"
echo "   ‚Ä¢ View Ollama logs:    tail -f /tmp/ollama.log"
echo "   ‚Ä¢ Restart Ollama:      pkill ollama && ollama serve &"
echo ""
echo "üí° Tips for macOS:"
echo "   ‚Ä¢ Ollama will use your Mac's GPU if available (Metal)"
echo "   ‚Ä¢ Recommended RAM: 8GB+ for smooth operation"
echo "   ‚Ä¢ Models are stored in ~/.ollama/models" 