{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (AutoTokenizer,\n",
    "                          DataCollatorForSeq2Seq,\n",
    "                          AutoModelForSeq2SeqLM,\n",
    "                          Seq2SeqTrainingArguments,\n",
    "                          Seq2SeqTrainer)\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "from processor.Processor import Processor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "INPUT_MAX_LENGTH = 1024\n",
    "TARGET_MAX_LENGTH = 128\n",
    "MODEL_ID = \"google/flan-t5-base\"\n",
    "OUTPUT_MODEL_ID = \"T5_spider\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID)\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "processor = Processor(tokenizer, rouge, INPUT_MAX_LENGTH, TARGET_MAX_LENGTH)\n",
    "\n",
    "\n",
    "# adding \"<\" to the tokenizer\n",
    "new_tokens = [\"<\"]\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# load dataset\n",
    "data = Dataset.load_from_disk('data/train_data')\n",
    "\n",
    "# process data\n",
    "data = data.map(processor.preprocess_function,\n",
    "                batched=True,\n",
    "                remove_columns=[\n",
    "                    'db_id', 'query', 'question', 'structure'\n",
    "                ])\n",
    "\n",
    "# train and test split\n",
    "data = data.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,\n",
    "                                       model=MODEL_ID,\n",
    "                                       label_pad_token_id=-100\n",
    "                                       )\n",
    "\n",
    "# training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_MODEL_ID + \"-checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_safetensors=False,\n",
    "    save_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=processor.target_max_length,\n",
    ")\n",
    "\n",
    "# trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# training\n",
    "print(\"Started training:\")\n",
    "trainer.train() # resume_from_checkpoint=True\n",
    "\n",
    "# saving the model\n",
    "save_model_name = OUTPUT_MODEL_ID\n",
    "trainer.model.save_pretrained(save_model_name)\n",
    "tokenizer.save_pretrained(save_model_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
